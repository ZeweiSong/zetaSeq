#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Created on Thu May 19 17:33:42 2022

Dereplicate a group of contigs

dereco prep -i /path/to/contigs.fa.gz -o /path/to/out/folder/ -c 1000000
dereco cacl -i /path/to/out/folder -t 4

He who loves to comment his code is unlikely to have bad luck.
@author: Song, Zewei
@contact: songzewei@genomics.cn
"""

import argparse
import hashlib
import json
import os
import random
import shutil
import string
import sys
import time
from datetime import datetime

from zetaSeq import io as seqIO
from zetaSeq import graph

def print_help():
    print('')
    print('DereCo: dereplicate a group of contigs')
    print('--------------------')
    print('Choose an option:')
    print('create     CREATE a working folder from a fromile or files in a directory.')
    print('devour     DEVOUR short sequences into longer ones passing a similarity threshold.')
    print('update     UPDATE a working directory with new files in the target path.')
    print('layout     LAYOUT graph assembly using all-versus-all overlaps.')
    print('status     Print out the STATUS of a working directory.')
    print('--------------------')
    print('Song, Zewei')
    print('songzewei@genomics.cn')
    print('')


def print_log(msg, log_file):
    date_string = datetime.now().strftime("%Y/%m/%d %H:%M:%S")
    print(msg)
    with open(log_file, 'a') as f:
        msg = msg.strip('\n')
        f.write('{0}\t{1}\n'.format(date_string, msg))


def check_dependency(tools):
    checker =[False] * len(tools)
    for index, item in enumerate(tools):
        if shutil.which(item) is None:
            print('[ERROR] {0} is not found in your ENV'.format(item))
            checker[index] = True
        else:
            print('[DereCo] {0} is ready'.format(item))
    if sum(checker) > 0:
        print('[DereCo] EXIT')
        sys.exit()


def get_random_string(length):
    letters = string.ascii_lowercase
    result_str = ''.join(random.choice(letters) for i in range(length))
    return result_str


def create(args_input, args_output, args_cutoff, args_minlen, args_sourmash):
    # Create an option dictionary
    options = {'cutoff': args_cutoff, 'minlen': args_minlen,
               'target': args_input, 'wd': args_output,
               'folder': False, 'final': '', 'finished': False,
               'threads': 1, 'cigar': False,
               'origin_size': 0, 'derep_size': 0,
               'origin_count': 0, 'origin_count': 0,
               'sig': args_sourmash, 'log': args_output + 'log',
               'big_head_son':'', 'small_head_dad':''}

    time_span = [0, 0] # Time recorder
    time_span[0] = time.time()
    
    # Check if the output folder exist
    if not args_output.endswith('/'): args_output += '/'
    if os.path.isdir(args_output):
        print('[ERROR] The output folder {0} already exist, please check it.'.format(args_output))
        sys.exit()
    else:
        os.mkdir(args_output)
        print('Working folder generated at: {0}'.format(args_output))
    
    # Create an empty log file
    with open(options['log'], 'w') as f:
        pass
    # Initiate the log file
    print_log('Dereco: dereplicate a group of contigs', options['log'])
    
    # Check dependency
    if args_sourmash: check_dependency(['seqtk', 'vsearch', 'sourmash'])
    else: check_dependency(['seqtk', 'vsearch'])
    
    # Check input file(s)
    fagz = []
    if os.path.isfile(args_input):  # Input is a file
        print_log('Origin file: {0}'.format(args_input), options['log'])
        if args_input.endswith('.fa') or args_input.endswith('.fa.gz'):
            pass
        else:
            print_log('[ERROR] The input file has to be .fa or .fa.gz', options['log'])
            sys.exit()
        target_file = args_input
    elif os.path.isdir(args_input):  # Input is a folder
        options['folder'] = True
        if not args_input.endswith('/'): args_input += '/'
        print_log('Origin directory: {0}'.format(args_input), options['log'])
        # Search in the target folder for all .fa.gz files, .fa will be ignored
        count_fa = 0
        for item in os.listdir(args_input):
            if item.endswith('.fa.gz'):
                fagz.append(args_input + item)
            elif item.endswith('.fa'):
                count_fa += 1
        if count_fa > 0:
            print_log('Found {0} .fa files in {1}, but will not add them '
                      'to the working directory'.format(count_fa, args_input), options['log'])
        if len(fagz) == 0:
            print_log('Found 0 .fa.gz files', options['log'])
            print_log('[DereCo] EXIT', options['log'])
            sys.exit()

    # Add files to .contigs
    if options['folder']:
        with open(args_output + '.contigs', 'w') as f:
            for item in fagz:
               f.write('{0}\n'.format(item))
    else:
        with open(args_output + '.contigs', 'w') as f:
            f.write('{0}\n'.format(args_input))

    # Rename the sequence names to avoid duplicated label
    if options['folder']:  # Target is a folder, will concat all its .fa.gz files first
        cmd = ['cat']
        cmd += [' '.join(fagz)]
        cmd += ['> ' + args_output + '_tmp.cat.fa.gz']
        target_file = args_output + '_tmp.cat.fa.gz'
        cmd = ' '.join(cmd)
        if len(fagz) >= 10:
            print_log('[DereCo] Concatenate {0} files ...'.format(len(fagz)), options['log'])
            print_log('[SHELL ] cat ' + ' '.join(fagz[:8]) + ' ... ' + fagz[10] + ' > ' + args_output + '_tmp.cat.fa.gz', options['log']) 
        else:
            print_log('[SHELL] ' + cmd, options['log'])
        os.system(cmd)
    else:
        target_file = args_input
    cmd = ['seqtk seq -L ' + str(args_minlen)]
    cmd += ['-C'] # -C is requied to remove all comments
    cmd += [target_file]
    cmd += ['|']
    cmd += ['seqtk rename -']
    cmd += ['contigs_']
    cmd += ['>']
    cmd += [args_output + '_tmp.cat.renamed.fa']
    cmd = ' '.join(cmd)
    print_log('[SEQTK] ' + cmd, options['log'])
    os.system(cmd)
    if options['folder']:
        os.system('rm ' + args_output + '_tmp.cat.fa.gz')  # remove the temp cat file

    # sourmash signature
    if options['sig']:
        print_log('[DereCo] Calculating sourmash sigature: k=21,k=31,k=51,scaled=1000,abund to {0}'.format(args_output + 'minhash_origin.sig'), options['log'])
        cmd = ['sourmash sketch dna']
        cmd += ['-p k=21,k=31,k=51,scaled=100000,abund']
        cmd += [args_output + '_tmp.cat.renamed.fa']
        cmd += ['-o ' + args_output + 'minhash_origin.sig']
        cmd += ['2> /dev/null']
        cmd = ' '.join(cmd)
        print_log('[SOURMASH] ' + cmd, options['log'])
        os.system(cmd)

    # Write large and small sequences to the working folder
    print_log('Length cutoff: {0}'.format(args_cutoff), options['log'])
    file_large = args_output + 'contigs.large.fa'
    file_small = args_output + 'contigs.small.fa'
    options['big_head_son'] = file_large
    options['small_head_dad'] = file_small

    # Cut input file with the cutoff
    cmd = ['vsearch --fastx_filter']
    cmd += [args_output + '_tmp.cat.renamed.fa']
    cmd += ['--fastq_minlen ' + str(args_cutoff)]
    cmd += ['--fastaout ' + file_large]
    cmd += ['--fastaout_discarded ' + file_small]
    cmd += ['--fasta_width 0']
    cmd += ['--quiet']
    cmd = ' '.join(cmd)
    print_log('[VSEARCH] ' + cmd, options['log'])
    os.system(cmd)
    os.remove(args_output + '_tmp.cat.renamed.fa')

    # Check if the large file is empty
    if os.stat(file_large).st_size == 0:
        print_log('[DereCo] The large file is empty, you may want to lower the cutoff.', options['log'])
        shutil.rmtree(args_output)
        print('[DereCo] {0} removed'.format(args_output))
        print('[DereCo] EXIT')
        sys.exit()
    else:
        # Write to FLOW
        with open(args_output + 'FLOW', 'w') as f:
            f.write('{0}\t{1}\n'.format(file_large, file_large))
            f.write('{0}\t{1}\n'.format(file_small, file_large))
            f.write('{0}\t{1}\n'.format(file_small, file_small))

    # Save the options.json file
    with open(args_output + 'options.json', 'w') as f:
        json.dump(options, f)

    print_log('[DereCo] Generated the LARGE file: {0}'.format(file_large), options['log'])
    print_log('[DereCo] Generated the SMALL file: {0}'.format(file_small), options['log'])
    print_log('[DereCo] Generated options.jaon', options['log'])
    print_log('[DereCo] Generated FLOW', options['log'])
    print_log('[DereCo] Next run: dereco devour -i {0} -t [threads]'.format(args_output), options['log'])
    time_span[1] = time.time()
    print_log('[DereCo] Used {0:,.0f} seconds'.format(time_span[1] - time_span[0]), options['log'])

    return None


# retuen a list of query names to be removed from alignments
def derep_paf(input_paf, offset):
    removal = []
    with open(input_paf, 'r') as f:
        for line in f:
            line = line.strip('\n').split('\t')
            if line[0] != line[5] and int(line[1]) < int(line[6]):  # alignment is short --> long
                ident = float(int(line[10]) / int(line[9]))  # This is the identity (similarity) of the alignment
                match = float(int(line[10]) / (int(line[1]) - offset))  # This is the percent of length matched on query
                if ident >= 0.99 and match >= 0.97:
                    removal.append((line[0], int(line[1])))
        removal = set(removal)
    return removal

# Dereplicate a query file against the ref file
def dereplicate(query, ref, threads, offset, cigar, options):
    # minimap2 asm5 ref query
    hash_object = hashlib.md5((query+ref+get_random_string(10)).encode())
    tmp_header = '__tmp__' + hash_object.hexdigest()
    cmd = ['minimap2']
    cmd += ['-x asm5']
    cmd += ['-t ' + str(threads)]
    if cigar: cmd += ['-c']
    cmd += [ref]
    cmd += [query]
    cmd += ['-o ' + tmp_header + '.paf']
    cmd += ['2> /dev/null']
    cmd = ' '.join(cmd)
    print_log('[MINIMAP2] ' + cmd, options['log'])
    os.system(cmd)

    # dereplicate query against ref
    removal = derep_paf(tmp_header + '.paf', offset)
    if len(removal) > 0:
        rm_names = set([x[0] for x in removal])
        rm_bps = sum([x[1] for x in removal])
        rm_max = max([x[1] for x in removal])
        rm_min = min([x[1] for x in removal])
        rm_ave = rm_bps / len(rm_names)
    else:
        rm_names = []
        rm_bps, rm_max, rm_min, rm_ave = 0, 0, 0, 0
    keep = [i[0] for i in seqIO.sequence(query) if i[0] not in rm_names]
    print_log('[DereCo] Removing {0:,} sequences: {1:,} bps, {2:,.0f} / seq, MAX = {3:,}, MIN = {4:,}'.format(len(removal), rm_bps, rm_ave, rm_max, rm_min), options['log'])
    with open(tmp_header + '_kplist', 'w') as f:
        f.write('{0}\n'.format('\n'.join(keep)))
    cmd = ['seqtk subseq']
    cmd += [query]
    cmd += [tmp_header + '_kplist']
    cmd += ['> ' + tmp_header + '_query.fa']
    cmd = ' '.join(cmd)
    print_log('[SEQTK] ' + cmd, options['log'])
    os.system(cmd)

    # replace old query with new query
    os.remove(query)
    shutil.move(tmp_header + '_query.fa', query)
    os.remove(tmp_header + '_kplist')
    os.remove(tmp_header + '.paf')
    print_log('[DereCo] {0} is dereplicated'.format(query), options['log'])

    return None

def get_size_count(input_seqs):
    size = 0
    count = 0
    for item in seqIO.sequence(input_seqs):
        count += 1
        size += len(item[1])
    return size, count


def devour(args_input, args_threads, args_offset, args_cigar):
    hash_object = hashlib.md5((args_input+get_random_string(10)).encode())
    tmp_header = '__tmp__' + hash_object.hexdigest()
    time_span = [0, 0]
    time_span[0] = time.time()
    if not args_input.endswith('/'): args_input += '/'
    if not os.path.isfile(args_input + 'options.json'):
        print('[ERROR] Did not find options.json, use "dereco create" to set up the working directory.')
        sys.exit()
    elif not os.path.isfile(args_input + 'FLOW'):
        print('[ERROR] Did not find FLOW, use "dereco create" to set up the working directory.')
    else:
        with open(args_input + 'options.json', 'r') as f:
            options = json.load(f)
            options['offset'] = args_offset
            if options['finished']:
                print_log('[DeroCo] This directory is marked as FINISHED', options['log'])
                sys.exit()
            else:
                flow = []
                with open(args_input + 'FLOW', 'r') as f:
                    for line in f:
                        line = line.strip('\n').split('\t')
                        flow.append(tuple(line))
    if options['sig']: check_dependency(['seqtk', 'minimap2', 'sourmash'])
    else: check_dependency(['seqtk', 'minimap2'])
    
    print_log('[DereCo] Cutoff: {0}'.format(options['cutoff']), options['log'])
    print_log('[DereCo] Minimum length: {0}'.format(options['minlen']), options['log'])
    print_log('[DereCo] Query length offset: {0}'.format(options['offset']), options['log'])
    options['cigar'] = args_cigar
    if options['cigar']: print('[DereCo] Cigar enabled in minimap2', options['log'])
    else: print_log('[DereCo] Cigar disabled in minimap2', options['log'])

    # Get the original file size and count
    sum0 = [0,0]
    c0 = [0, 0]
    sum0[0], c0[0] = get_size_count(options['big_head_son'])
    try:
        sum0[1], c0[1] = get_size_count(options['small_head_dad'])
    except IndexError:
        sum0[1], c0[1] = 0, 0
    
    print_log('[DereCo] FLOW started:', options['log'])
    for item in flow:
        query = item[0]
        ref = item[1]
        if not os.path.isfile(query):
            print_log('[ERROR] {0} is missing'.format(query), options['log'])
            print_log('[DereCo] EXIT', options['log'])
            sys.exit()
        elif not os.path.isfile(ref):
            print_log('[ERROR] {0} is missing'.format(ref), options['log'])
            print_log('[DereCo] EXIT', options['log'])
            sys.exit()
        print_log('[DereCo] FLOW: {0} -> {1}'.format(query, ref), options['log'])
        sum_query, count_query = get_size_count(query)
        dereplicate(query, ref, args_threads, args_offset, args_cigar, options)
        sum1, c1 = get_size_count(query)
        print_log('[DereCo] Dereplicated {4}: {0:,} -> {1:,} (bps) / {2:,} -> {3:,} (count)'.format(sum_query, sum1, count_query, c1, query), options['log'])

    # concat all and rename with prefix nrc_ aka Non-redundant contigs
    cmd = ['cat']
    cmd += [options['big_head_son']]
    cmd += [options['small_head_dad']]
    cmd += ['| seqtk rename - nrc_ > ' + args_input + 'dereco_final.fa']
    cmd = ' '.join(cmd)
    print_log('[SHELL] ' + cmd, options['log'])
    os.system(cmd)

    options['final'] = args_input + 'dereco_final.fa'
    options['finished'] = True

    if options['sig']:
        cmd = ['sourmash sketch dna']
        cmd += ['-p k=21,k=31,k=51,scaled=100000,abund']
        cmd += [options['final']]
        cmd += ['-o ' + args_input + 'minhash_derep.sig']
        cmd += ['2> /dev/null']
        cmd = ' '.join(cmd)
        print_log('[DereCo] Calculating minhash sigature for the dereplicated file', options['log'])
        print_log('[SOURMASH] ' + cmd, options['log'])
        os.system(cmd)

    if os.path.isfile(args_input + 'dereco_final.fa'):
        os.system('touch ' + args_input + 'FINISHED')
        os.remove(options['big_head_son'])
        os.remove(options['small_head_dad'])
    
    sum1, c1 = get_size_count(args_input + 'dereco_final.fa')
    count = 0
    with open(args_input + '.contigs', 'r') as f:
        for line in f:
            count += 1
    print_log('\n\t---Report---', options['log'])
    print_log('Number of contigs: {0}'.format(count), options['log'])
    print_log('Compression rate: {0:.2f}'.format(sum(sum0) / (sum1)), options['log'])
    print_log('Origin size / Derep size: {0:,} / {1:,} --> {2:,} bps'.format(sum(sum0), sum1, sum(sum0) - sum1), options['log'])
    print_log('Origin count / Derep count: {0:,} / {1:,} --> {2:,} seqs'.format(sum(c0), c1, sum(c0) - c1), options['log'])
    options['origin_size'] = sum(sum0)
    options['derep_size'] = sum1
    options['origin_count'] = sum(c0)
    options['derep_count'] = c1
    options['cigar'] = args_cigar
    if options['folder']:
        print_log('To update: dereco update -i {0} -t [threads]'.format(options['wd']), options['log'])
    with open(args_input + 'options.json', 'w') as f:
        json.dump(options, f)
    time_span[1] = time.time()
    print_log('[DereCo] Used: {0:,.0f} seconds'.format(time_span[1] - time_span[0]), options['log'])

    return None

def update(args_input):
    time_span = [0, 0]
    time_span[0] = time.time()
    # Check options.json
    if os.path.isfile(args_input + 'options.json'):
        with open(args_input + 'options.json', 'r') as f:
            options = json.load(f)
    else:
        print('[ERROR] options.json is missing in {0}'.format(args_input))
        print('[DereCo] EXIT')
        sys.exit()
    count = 0
    contigs_finished = []
    with open(args_input + '.contigs', 'r') as f:
        for line in f:
            contigs_finished.append(line.strip('\n'))
            count += 1
    contigs_add = []
    if not options['folder']:  # Target is not a folder
        print_log('[ERROR] Target is a file not a directory.', options['log'])
        print_log('[DereCo] EXIT', options['log'])
    elif not options['finished']:  # Input directory is NOT finished
        print_log('[ERROR] Input directory is not finished.', options['log'])
        print_log('[DereCo] run: dereco calculate -i {0} -t [threads]'.format(options['wd']), options['log'])
    else:
        print_log('[DereCo] {0} is a FINISHED working directory'.format(options['wd']), options['log'])
        print_log('[DereCo] Update contigs (.fa.gz) in {0}'.format(options['target']), options['log'])
        print_log('[DereCo] Finished contigs: {0}'.format(count), options['log'])
        contigs_finished = set([i.split('/')[-1] for i in contigs_finished])
        for item in os.listdir(options['target']):
            if item.endswith('.fa.gz'):
                if item not in contigs_finished:
                    contigs_add.append(options['target'] + item)
                    print_log('[DereCo] Add new contig: {0}'.format(item), options['log'])
        if len(contigs_add) > 0:
            count_added = 0
            print_log('[DereCo] Added {0} new contig(s)'.format(len(contigs_add)), options['log'])
            with open(args_input + '.contigs', 'a') as f:
                for item in contigs_add:
                    f.write('{0}\n'.format(item))
                    count_added += 1
        else:
            print_log('[DereCo] No new contigs found in {0}'.format(options['target']), options['log'])
            print_log('[DereCo] EXIT', options['log'])
            sys.exit()

    # concat new contigs as new small file
    options['small_head_dad'] = options['wd'] + 'contigs.small.fa'
    cmd = ['cat']
    cmd += [' '.join(contigs_add)]
    cmd += ['| seqtk seq -L ' + str(options['minlen']) + ' -C | seqtk rename - added_ > ' + options['small_head_dad']] # need to use seqtk seq -C to remove comments in labels
    cmd = ' '.join(cmd)
    print_log('[SHELL] ' + cmd, options['log'])
    os.system(cmd)

    # Set large_file = final
    options['big_head_son'] = options['wd'] + 'contigs.large.fa'
    cmd = ['mv']
    cmd += [options['final']]
    cmd += [options['big_head_son']]
    cmd = ' '.join(cmd)
    print_log('[SHELL] ' + cmd, options['log'])
    shutil.move(options['final'], options['big_head_son'])
    options['final'] = ''
   
    # Write a new FLOW to update
    with open(args_input + 'FLOW', 'w') as f:
        f.write('{0}\t{1}\n'.format(options['small_head_dad'], options['small_head_dad']))
        f.write('{0}\t{1}\n'.format(options['small_head_dad'], options['big_head_son']))
        f.write('{0}\t{1}\n'.format(options['big_head_son'], options['small_head_dad']))

    # Turn off finished flag
    options['finished'] = False
    with open(args_input + 'options.json', 'w') as f:
        json.dump(options, f)
    if os.path.isfile(options['wd'] + 'FINISHED'):
        os.remove(options['wd'] + 'FINISHED')

    # Print devour command
    print_log('[DereCo] Working directory set up ready', options['log'])
    print_log('[DereCo] WD = {0}'.format(options['wd']), options['log'])
    print_log('[DereCo] Devoured contigs: {0}'.format(count), options['log'])
    print_log('[DereCo] Added contigs: {0}'.format(count_added), options['log'])
    print_log('[DereCo] Large file = {0}'.format(options['big_head_son']), options['log'])
    print_log('[DereCo] Small file = {0}'.format(options['small_head_dad']), options['log'])
    print_log('[DereCo] New FLOW is ready', options['log'])
    print_log('[DereCo] NEXT run: dereco devour -i {0} -t [threads]'.format(options['wd']), options['log'])
    time_span[1] = time.time()
    print_log('[DereCo] Used: {0:,.0f}'.format(time_span[1] - time_span[0]), options['log'])

    return None

def status(args_input):
    print('[DereCo] Checking {0}'.format(args_input))
    if not args_input.endswith('/'): args_input += '/'
    if not os.path.isdir(args_input):
        print('[ERROR] {0} not exist')
        print('[DereCo] EXIT')
        sys.exit()
    else:
        if not os.path.isfile(args_input + 'options.json'):
            print('[ERROR] options.json not exist, {0} is not a working directory'.format(args_input))
            print('[DereCo] EXIT')
            sys.exit()
        else:
            with open(args_input + 'options.json', 'r') as f:
                options = json.load(f)
            print('[DereCo] Loading options.json')
        if not os.path.isfile(args_input + '.contigs'):
            print('[ERROR] .contigs not exist, {0} is not a working directory'.format(args_input))
            print('[DereCo] EXIT')
            sys.exit()
        else:
            count = 0
            with open(args_input + '.contigs', 'r') as f:
                for line in f:
                    count +=1

    if options['finished']:
        print('[DereCo] Project is FINISHED')
        print('[DereCo] Project path: {0}'.format(options['wd']))
        print('[DereCo] Non-redundant contigs set in {0}'.format(options['final']))
        print('[DereCo] Compressed from {0} files'.format(count))
        print('[DereCo] Cutoff = {0:,}'.format(options['cutoff']))
        print('[DereCo] Minimum length = {0:,}'.format(options['minlen']))
        print('[DereCo] Query length offset: {0}'.format(options['offset']))
        if options['cigar']: print('[DereCo] Cigar enabled in minimap2')
        else: print('[DereCo] Ciagr disabled in minimap2')
        print('[DereCo] Origin versus derep: {0:,} / {1:,} (bps)'.format(options['origin_size'], options['derep_size']))
        print('[DereCo] Compression rate: {0:.2f}'.format(options['origin_size'] / options['derep_size']))        
        print('[DereCo] Path to update: {0}'.format(options['target']))
    else:
        print('[DereCo] Project is NOT finished')
        print('[DereCo] Project path: {0}'.format(options['wd']))
        print('[DereCo] Cutoff = {0:,}'.format(options['cutoff']))
        print('[DereCo] Minimum length = {0:,}'.format(options['minlen']))
        print('[DereCo] Source of contigs: {0}'.format(options['target']))
        print('[DereCo] Added {0} files'.format(count))
    with open(args_input + 'FLOW', 'r') as f:
        print('[DereCo] FLOW')
        for line in f:
            line = line.strip('\n').split('\t')
            print('[DereCo] FLOW: {0} -> {1}'.format(line[0], line[1]))
    if options['finished'] and options['sig']:
        cmd = ['sourmash compare']
        cmd += ['-k 21']
        cmd += [args_input + 'minhash_origin.sig']
        cmd += [args_input + 'minhash_derep.sig']
        cmd = ' '.join(cmd)
        print('[SOURMASH] ' + cmd)
        os.system(cmd)


def layout(args_input, args_output, args_threads, args_min_len, args_id, args_match, args_offset):
    # Create an option dictionary
    options = {'cutoff': 0, 'minlen': 0, 'ava_minlen': args_min_len,
               'target': args_input, 'wd': args_output,
               'folder': False, 'final': '', 'finished': False,
               'threads': 1, 'cigar': False,
               'origin_size': 0, 'derep_size': 0,
               'origin_count': 0, 'origin_count': 0,
               'sig': False, 'log': args_output + 'log',
               'big_head_son': '', 'small_head_dad': ''}

    time_span = [0, 0]  # Time recorder
    time_span[0] = time.time()

    # Check if the output folder exist
    if not args_output.endswith('/'): args_output += '/'
    if os.path.isdir(args_output):
        print('[ERROR] The output folder {0} already exist, please check it.'.format(args_output))
        sys.exit()
    else:
        os.mkdir(args_output)
        print('Working folder generated at: {0}'.format(args_output))
        with open(args_output + 'options.json', 'w') as f:
            json.dump(options, f)

    # Create an empty log file
    with open(options['log'], 'w') as f:
        pass
    # Initiate the log file
    print_log('Dereco: dereplicate a group of contigs', options['log'])

    # Check options.json
    if os.path.isfile(args_output + 'options.json'):
        with open(args_output + 'options.json', 'r') as f:
            options = json.load(f)
    else:
        print('[ERROR] options.json is missing in {0}'.format(args_input))
        print('[DereCo] EXIT')
        sys.exit()
    check_dependency(['seqtk', 'minimap2'])

    # Initiate the log file
    print_log('[DereCo] LAYOUT: overlap LAYOUT assembly of a dereplicated dataset', options['log'])
    hash_object = hashlib.md5((args_input+get_random_string(10)).encode())
    tmp_header = '__tmp__' + hash_object.hexdigest()

    # Cut the input at the minimum length
    print_log('[DereCo] Cut {0} at minimum length {1}'.format(args_input, args_min_len), options['log'])
    tmp_cutlen = tmp_header + '_minlen.fa'
    cmd = ['seqtk seq']
    cmd += ['-L ' + str(args_min_len)]
    cmd += [options['target']]
    cmd += ['> ' + tmp_cutlen]
    cmd = ' '.join(cmd)
    print_log('[SEQTK] ' + cmd, options['log'])
    os.system(cmd)

    # minimap2 ava-ont -c
    print_log('[DereCo] Minimap2 all-versus-all alignment', options['log'])
    tmp_ovlp = tmp_header + '_ovlp.paf'
    cmd = ['minimap2']
    cmd += ['-x ava-ont']
    cmd += ['-c']
    cmd += ['-t ' + str(args_threads)]
    cmd += [tmp_cutlen]
    cmd += [tmp_cutlen]
    cmd += ['> ' + tmp_ovlp]
    cmd = ' '.join(cmd)
    print_log('[MINIMAP2] ' + cmd, options['log'])
    os.system(cmd)
    os.remove(tmp_cutlen)

    # Overlap layout assemble
    print_log('[DereCo] Overlap layout assembly', options['log'])
    print_log('[DereCo] LAYOUT: tail offset = {0}, alignment ident = {1}, match length = {2}'
              .format(args_offset, args_id, args_match), options['log'])
    ovlp = graph.overlap_list(tmp_ovlp, t=args_offset, id=args_id, m=args_match)
    if len(ovlp) == 0:
        print_log('ERROR] Did not find significant overlap, you may want to change the matching parameters', options['log'])
        print_log('[DereCo] EXIT', options['log'])
        sys.exit()
    else:
        print_log('[DereCo] Loaded {0} all-versus-all align events'.format(len(ovlp)), options['log'])
    g = graph.overlap_layout_graph(ovlp, log=False)
    print_log('[DereCo] Overlaps assembled into {0] subgraphs, DAGs: {1}, DCGs: {2}'
              .format(len(g['dag']) + len(g['gcg']), len(g['dag']), len(g['dcg'])), options['log'])
    g_trim = {'dag':[], 'dcg':[]}  # a new list for saving trimmed subgraphs, may be longer than variable g
    for item in g['dag']:
        g_tmp = graph.trim_ovlp_graph(item)
        g_trim['dag'] += g_tmp['dag']
        g_trim['dcg'] += g_tmp['dcg']
    for item in g['dcg']:
        g_tmp = graph.trim_ovlp_graph(item)
        g_trim['dag'] += g_tmp['dag']
        g_trim['dcg'] += g_tmp['dcg']
    print_log('[DereCo] Graph reduce and remove into {0] subgraphs, DAGs: {1}, DCGs: {2}'
              .format(len(g['dag']) + len(g['gcg']), len(g['dag']), len(g['dcg'])), options['log'])

    ovlp_dict = graph.overlap_dict(ovlp)
    seq_dict = graph.sequence_dict(options['target'])
    count = 0
    for item in g_trim['dag']:
        seq = graph.ovlp_sequence(item, ovlp_dict, seq_dict)
        seq[0] = 'dag_' + str(count) + ';' + seq[0]
        seq_dict['dag'].append(seq)
        count += 1
    count = 0
    for item in g_trim['dcg']:
        seq = graph.ovlp_sequence(item, ovlp_dict, seq_dict)
        seq[0] = 'dcg_' + str(count) + ';' + seq[0]
        seq_dict['dcg'].append(seq)
        count += 1

    # Remove ctgs from origin sequence
    rmlist = graph.rmlist_graph(g_trim['dag'] + g_trim['dcg'])  # sequences to be removed
    ctg_list = [i[0] for i in seqIO.sequence(options['final'])]  # origin contig names
    keep_list = [i for i in ctg_list if i not in rmlist]  # sequences to be kept
    keep_list_file = tmp_header + '_kplist'  # Write to the keep list file
    with open(keep_list_file, 'w') as f:
        for item in keep_list:
            f.write('{0}\n'.format(item))
    print_log('[DereCo] Write {0} unassembled sequences'.format(len(keep_list)), options['log'])
    cmd = ['seqtk subseq']
    cmd += [args_input]
    cmd += [keep_list_file]
    cmd += ['> ' + args_output + 'dereco_final.unassembled.fa']
    cmd = ' '.join(cmd)
    print_log('[SEQTK] ' + cmd, options['log'])
    os.system(cmd)

    # Write trimmed sub-graphs to FASTA files
    c_dag = seqIO.write_seqs(seq_dict['dag'], args_output + 'dereco_final.dag.fa')
    c_dcg = seqIO.write_seqs(seq_dict['dcg'], args_output + 'dereco_final.dcg.fa')

    # Write origin graphs and trimmed graphs to GFA format
    gfa_origin = graph.graph_to_gfa(g['dag'] + g['dcg'], options['final'])
    gfa_trimmed = graph.graph_to_gfa(g_trim['dag'] + g_trim['dcg'], options['final'])
    with open(args_input + 'dereco_final.origin.gfa', 'w') as f:
        for line in gfa_origin:
            f.write('{0}/n'.format('\t'.join(line)))
    with open(args_input + 'dereco_final.trimmed.gfa', 'w') as f:
        for line in gfa_trimmed:
            f.write('{0}/n'.format('\t'.join(line)))

    time_span[1] = time.time()
    print('[DereCo] Used: {0:,.0f} seconds'.format(time_span[1] - time_span[0]))
    
    return None


if len(sys.argv[:]) == 1:
    option = ''
else:
    option = sys.argv[1]
if option == 'create':
    print('CREATE a working folder from a file or files in a directory')
    parser = argparse.ArgumentParser(prog='dereco create')
    parser.add_argument('-i', '--input', help='Path to the original contigs group file.')
    parser.add_argument('-o', '--output', help='Path to the output folder.')
    parser.add_argument('-c', '--cutoff', type=int, default=1000000, help='Length cutff to divide the input.')
    parser.add_argument('-m', '--minlen', type=int, default=2000, help='Minimum length to keep a sequence.')
    parser.add_argument('-s', '--sourmash', action='store_true', help='Indicator to calculate minHash siganture using sourmash.')
    if len(sys.argv[:]) == 2:
        parser.print_help()
    else:
        args = parser.parse_args(sys.argv[2:])
        create(args.input, args.output, args.cutoff, args.minlen, args.sourmash)
elif option in ('devour', 'devo'):
    print('DEVOUR short sequences into longer ones passing a similarity threshold')
    parser = argparse.ArgumentParser(prog='dereco calculate')
    parser.add_argument('-i', '--input', help='Path to the prep output folder.')
    parser.add_argument('-f', '--offset', default=0, type=int, help='Offset for the match length')
    parser.add_argument('-c', '--cigar', action='store_true', help='Use more accurate mode of minimap2, but slower.')
    parser.add_argument('-t', '--threads', type=int, default=2, help='Number of threads')
    if len(sys.argv[:]) == 2:
        parser.print_help()
    else:
        args = parser.parse_args(sys.argv[2:])
        devour(args.input, args.threads, args.offset, args.cigar)
elif option == 'update':
    print('UPDATE a working directory for new contigs in the target path')
    parser = argparse.ArgumentParser(prog='dereco update')
    parser.add_argument('-i', '--input', help='Path to the stat output folder.')
    if len(sys.argv[:]) == 2:
        parser.print_help()
    else:
        args = parser.parse_args(sys.argv[2:])
        update(args.input)
elif option == 'layout':
    print('LAYOUT graph assembly using all-versus-all overlaps')
    parser = argparse.ArgumentParser(prog='dereco layout')
    parser.add_argument('-i', '--input', help='Path to the input directory containing dereplicated contigs.')
    parser.add_argument('-o', '--output', help='Path to the output directory.')
    parser.add_argument('-t', '--threads', type=int, default=2, help='Number of threads.')
    parser.add_argument('-l', '--min_len', type=int, default=5000, help='Minimum length to cut for entering the all-vs-all alignment.')
    parser.add_argument('-d', '--id', type=float, default=0.99, help='Identity cutoff for alignments.')
    parser.add_argument('-m', '--match', type=int, default=2000, help='Minimum overlap length for an alignment.')
    parser.add_argument('-f', '--offset', type=int, default=300, help='Maximum tail offset for an overlap.')
    if len(sys.argv[:]) == 2:
        parser.print_help()
    else:
        args = parser.parse_args(sys.argv[2:])
        layout(args.input, args.output, args.threads, args.min_len, args.id, args.match, args.offset)
elif option in ('status', 'stat'):
    print('STATUS of a working directory')
    parser = argparse.ArgumentParser(prog='dereco status')
    parser.add_argument('-i', '--input', help='Path to the working directory.')
    if len(sys.argv[:]) == 2:
        parser.print_help()
    else:
        args = parser.parse_args(sys.argv[2:])
        status(args.input)
else:
    print_help()
